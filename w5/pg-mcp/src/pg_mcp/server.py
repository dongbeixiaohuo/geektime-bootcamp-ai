"""FastMCP server for PostgreSQL natural language query interface.

This module implements the MCP server using FastMCP, exposing the query
functionality as an MCP tool. It includes complete lifespan management for
initializing and cleaning up all components.
"""

from collections.abc import AsyncIterator
from contextlib import asynccontextmanager
from typing import Any

from asyncpg import Pool
from mcp.server.fastmcp import FastMCP

from pg_mcp.cache.schema_cache import SchemaCache
from pg_mcp.config.settings import Settings
from pg_mcp.db.pool import close_pools, create_pool
from pg_mcp.models.query import QueryRequest, QueryResponse, ReturnType
from pg_mcp.observability.logging import configure_logging, get_logger
from pg_mcp.observability.metrics import MetricsCollector
from pg_mcp.observability.tracing import request_context
from pg_mcp.resilience.circuit_breaker import CircuitBreaker
from pg_mcp.resilience.rate_limiter import MultiRateLimiter
from pg_mcp.services.orchestrator import QueryOrchestrator
from pg_mcp.services.result_validator import ResultValidator
from pg_mcp.services.sql_executor import SQLExecutor
from pg_mcp.services.sql_generator import SQLGenerator
from pg_mcp.services.sql_validator import SQLValidator

logger = get_logger(__name__)

# Global state for lifespan management
_settings: Settings | None = None
_pools: dict[str, Pool] | None = None
_schema_cache: SchemaCache | None = None
_orchestrator: QueryOrchestrator | None = None
_metrics: MetricsCollector | None = None
_circuit_breaker: CircuitBreaker | None = None
_rate_limiter: MultiRateLimiter | None = None


@asynccontextmanager
async def lifespan(_app: FastMCP) -> AsyncIterator[None]:  # type: ignore[type-arg]
    """Lifespan context manager for server initialization and cleanup.

    This function manages the complete lifecycle of the MCP server:

    Startup:
        1. Load configuration from Settings
        2. Configure logging
        3. Create database connection pools
        4. Load schema cache for all databases
        5. Initialize metrics collector
        6. Create service components (generators, validators, executors)
        7. Initialize resilience components (circuit breaker, rate limiter)
        8. Create query orchestrator
        9. Start metrics HTTP server (optional)

    Shutdown:
        1. Stop schema auto-refresh (if enabled)
        2. Close all database connection pools
        3. Stop metrics HTTP server (if running)

    Yields:
        None

    Example:
        >>> async with lifespan():
        ...     # Server is running with all components initialized
        ...     pass
    """
    global _settings, _pools, _schema_cache, _orchestrator, _metrics
    global _circuit_breaker, _rate_limiter

    # Setup emergency logging before anything else
    import logging
    import sys
    from pathlib import Path

    emergency_log = Path.cwd() / "mcp_server_startup.log"
    emergency_handler = logging.FileHandler(emergency_log, encoding="utf-8", mode="w")
    emergency_handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    logging.getLogger().addHandler(emergency_handler)
    logging.getLogger().setLevel(logging.DEBUG)

    logger.info("Starting PostgreSQL MCP Server initialization...")
    logger.info(f"Emergency log: {emergency_log}")
    logger.info(f"Python version: {sys.version}")
    logger.info(f"Working directory: {Path.cwd()}")

    try:
        # 1. Load Settings
        logger.info("Loading configuration...")
        try:
            _settings = Settings()
            logger.info("Settings loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load settings: {e}", exc_info=True)
            raise

        # 2. Configure logging
        logger.info("Configuring logging...")
        try:
            configure_logging(
                level=_settings.observability.log_level,
                log_format=_settings.observability.log_format,
                enable_sensitive_filter=True,
            )
            logger.info("Logging configured")
        except Exception as e:
            logger.error(f"Failed to configure logging: {e}", exc_info=True)
            raise

        # Force file logging for debugging (Re-add after configure_logging cleared it)
        file_handler = logging.FileHandler("mcp_debug.log", encoding='utf-8')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        logging.getLogger().addHandler(file_handler)
        logging.getLogger().setLevel(logging.DEBUG)

        logger.info(
            "Configuration loaded",
            extra={
                "environment": _settings.environment,
                "log_level": _settings.observability.log_level,
            },
        )

        # 3. Create database connection pools
        logger.info("Creating database connection pools...")
        _pools = {}

        # Import os for environment variable access
        import os
        from pg_mcp.config.settings import DatabaseConfig

        # Primary database (from DATABASE_* env vars)
        db1_config = DatabaseConfig(
            host=os.getenv("DATABASE_HOST", "localhost"),
            port=int(os.getenv("DATABASE_PORT", "5432")),
            name=os.getenv("DATABASE_NAME", "postgres"),
            user=os.getenv("DATABASE_USER", "postgres"),
            password=os.getenv("DATABASE_PASSWORD", ""),
        )
        pool1 = await create_pool(db1_config)
        _pools[db1_config.name] = pool1
        logger.info(
            f"Created connection pool for database '{db1_config.name}'",
            extra={
                "min_size": db1_config.min_pool_size,
                "max_size": db1_config.max_pool_size,
            },
        )

        # Second database (from DATABASE2_* env vars, if configured)
        if os.getenv("DATABASE2_NAME"):
            db2_config = DatabaseConfig(
                host=os.getenv("DATABASE2_HOST", os.getenv("DATABASE_HOST", "localhost")),
                port=int(os.getenv("DATABASE2_PORT", os.getenv("DATABASE_PORT", "5432"))),
                name=os.getenv("DATABASE2_NAME"),
                user=os.getenv("DATABASE2_USER", os.getenv("DATABASE_USER", "postgres")),
                password=os.getenv("DATABASE2_PASSWORD", os.getenv("DATABASE_PASSWORD", "")),
            )
            pool2 = await create_pool(db2_config)
            _pools[db2_config.name] = pool2
            logger.info(
                f"Created connection pool for database '{db2_config.name}'",
                extra={
                    "min_size": db2_config.min_pool_size,
                    "max_size": db2_config.max_pool_size,
                },
            )

        # 4. Load Schema cache
        logger.info("Initializing schema cache...")
        _schema_cache = SchemaCache(_settings.cache)

        # Skip schema loading during startup to prevent timeout
        logger.info("Skipping initial schema loading to improve startup time...")
        # for db_name, pool in _pools.items():
        #     logger.info(f"Loading schema for database '{db_name}'...")
        #     schema = await _schema_cache.load(db_name, pool)
        #     logger.info(
        #         f"Schema loaded for '{db_name}'",
        #         extra={
        #             "tables": len(schema.tables),
        #         },
        #     )

        # Optional: Start schema auto-refresh
        # Disabled by default to avoid unnecessary background tasks
        # Uncomment to enable:
        # if _settings.cache.enabled:
        #     logger.info("Starting schema auto-refresh...")
        #     await _schema_cache.start_auto_refresh(
        #         interval_minutes=60,  # Refresh every hour
        #         pools=_pools,
        #     )

        # 5. Initialize metrics collector
        logger.info("Initializing metrics collector...")
        _metrics = MetricsCollector()

        # Start metrics HTTP server if enabled
        if _settings.observability.metrics_enabled:
            from prometheus_client import start_http_server

            start_http_server(_settings.observability.metrics_port)
            logger.info(f"Metrics server started on port {_settings.observability.metrics_port}")

        # 6. Create service components
        logger.info("Initializing service components...")

        # SQL Generator
        sql_generator = SQLGenerator(_settings.openai)

        # SQL Validator
        sql_validator = SQLValidator(
            config=_settings.security,
            blocked_tables=_settings.security.blocked_tables,
            blocked_columns=_settings.security.blocked_columns,
            explain_policy=_settings.security.explain_policy,
        )

        # SQL Executor (create one per database)
        sql_executors: dict[str, SQLExecutor] = {}
        for db_name, pool in _pools.items():
            executor = SQLExecutor(
                pool=pool,
                security_config=_settings.security,
                db_config=_settings.database,
                metrics=_metrics,
            )
            sql_executors[db_name] = executor
            logger.info(f"Created SQL executor for database '{db_name}'")

        # Result Validator
        result_validator = ResultValidator(
            openai_config=_settings.openai,
            validation_config=_settings.validation,
        )

        # 7. Initialize resilience components
        logger.info("Initializing resilience components...")

        # Circuit Breaker for LLM calls
        _circuit_breaker = CircuitBreaker(
            failure_threshold=_settings.resilience.circuit_breaker_threshold,
            recovery_timeout=_settings.resilience.circuit_breaker_timeout,
        )

        # Rate Limiter
        _rate_limiter = MultiRateLimiter(
            query_limit=10,  # Can be made configurable
            llm_limit=5,  # Can be made configurable
        )

        # 8. Create QueryOrchestrator
        logger.info("Creating query orchestrator...")
        _orchestrator = QueryOrchestrator(
            sql_generator=sql_generator,
            sql_validator=sql_validator,
            sql_executors=sql_executors,
            result_validator=result_validator,
            schema_cache=_schema_cache,
            pools=_pools,
            resilience_config=_settings.resilience,
            validation_config=_settings.validation,
            rate_limiter=_rate_limiter,
            metrics=_metrics,
        )

        logger.info("PostgreSQL MCP Server initialization complete!")
        logger.info(
            "Server ready to accept requests",
            extra={
                "databases": list(_pools.keys()),
                "cache_enabled": _settings.cache.enabled,
                "metrics_enabled": _settings.observability.metrics_enabled,
            },
        )

        # Yield to run the server
        yield

    except Exception as e:
        logger.exception("Fatal error during server initialization")
        # Also write to emergency log
        import traceback
        error_detail = f"FATAL ERROR: {e}\n{traceback.format_exc()}"
        logger.error(error_detail)

        # Write to separate error file
        try:
            from pathlib import Path
            error_file = Path.cwd() / "mcp_fatal_error.log"
            with open(error_file, "w", encoding="utf-8") as f:
                f.write(error_detail)
            logger.error(f"Error details written to {error_file}")
        except Exception:
            pass

        raise e

    finally:
        # Shutdown sequence
        logger.info("Starting PostgreSQL MCP Server shutdown...")

        # Stop schema auto-refresh with timeout
        if _schema_cache is not None:
            try:
                import asyncio
                await asyncio.wait_for(
                    _schema_cache.stop_auto_refresh(),
                    timeout=3.0
                )
                logger.info("Schema auto-refresh stopped")
            except asyncio.TimeoutError:
                logger.warning("Schema auto-refresh stop timed out")
            except Exception as e:
                logger.warning(f"Error stopping schema auto-refresh: {e!s}")

        # Close database connection pools with timeout
        if _pools is not None:
            try:
                # Use 5 second timeout for graceful shutdown
                await close_pools(_pools, timeout=5.0)
                logger.info("Database connection pools closed")
            except Exception as e:
                logger.error(f"Error closing connection pools: {e!s}")

        logger.info("PostgreSQL MCP Server shutdown complete")


# Create FastMCP server instance with lifespan
mcp = FastMCP("pg-mcp", lifespan=lifespan)


@mcp.tool()
async def query(
    question: str,
    database: str | None = None,
    return_type: str = "result",
) -> dict[str, Any]:
    """Execute a natural language query against PostgreSQL database.

    This tool converts natural language questions into SQL queries and executes
    them against the specified PostgreSQL database. It includes comprehensive
    security validation, result verification, and error handling.

    Args:
        question: Natural language description of the query.
            Examples:
                - "How many users registered in the last 30 days?"
                - "Show me the top 10 products by revenue"
                - "What is the average order value by country?"

        database: Target database name (optional if only one database is configured).
            If not specified and only one database is available, it will be
            automatically selected.

        return_type: Type of result to return.
            Options:
                - "sql": Return only the generated SQL query without executing it
                - "result": Execute the query and return results (default)

    Returns:
        dict: Query response containing:
            - success (bool): Whether the query succeeded
            - generated_sql (str): The generated SQL query
            - data (dict): Query results if executed (columns, rows, row_count, etc.)
            - error (dict): Error information if query failed
            - confidence (int): Confidence score (0-100) for result quality
            - tokens_used (int): Number of LLM tokens consumed

    Examples:
        >>> # Get query results
        >>> result = await query(
        ...     question="How many active users are there?",
        ...     return_type="result"
        ... )
        >>> print(result["data"]["rows"])

        >>> # Get SQL only
        >>> result = await query(
        ...     question="Count all products",
        ...     return_type="sql"
        ... )
        >>> print(result["generated_sql"])

    Raises:
        This function does not raise exceptions. All errors are captured and
        returned in the response with success=False and error details.

    Security:
        - Only SELECT queries are allowed (no INSERT, UPDATE, DELETE, DROP, etc.)
        - Dangerous PostgreSQL functions are blocked (pg_sleep, file operations, etc.)
        - Query execution timeout is enforced
        - Row count limits prevent memory exhaustion
        - All queries run in read-only transactions
    """
    global _orchestrator

    if _orchestrator is None:
        return {
            "success": False,
            "error": {
                "code": "SERVER_NOT_INITIALIZED",
                "message": "Server not initialized properly",
                "details": None,
            },
        }

    # Validate return_type
    if return_type not in ("sql", "result"):
        return {
            "success": False,
            "error": {
                "code": "INVALID_PARAMETER",
                "message": f"Invalid return_type: '{return_type}'. Must be 'sql' or 'result'.",
                "details": {"return_type": return_type},
            },
        }

    # Build request
    try:
        async with request_context() as request_id:
            # Note: request_id is implicitly available via context, but we can also pass it explicitly
            # if we add it to QueryRequest model. For now, orchestrator gets it from context.
            request = QueryRequest(
                question=question,
                database=database,
                return_type=ReturnType(return_type),
            )
            # Inject request_id if QueryRequest supports it (dynamic check)
            if hasattr(request, "request_id"):
                setattr(request, "request_id", request_id)

            # Execute query through orchestrator
            response: QueryResponse = await _orchestrator.execute_query(request)
            result = response.to_dict()
            return result

    except Exception as e:
        return {
            "success": False,
            "error": {
                "code": "INVALID_REQUEST",
                "message": f"Invalid request parameters: {e!s}",
                "details": {"error": str(e)},
            },
        }


if __name__ == "__main__":
    """Run the server when executed directly."""
    import anyio

    anyio.run(mcp.run_stdio_async)
